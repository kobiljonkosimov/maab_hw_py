{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weather.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "rows = soup.find(\"table\").find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "weather_data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    day = cols[0].text\n",
    "    temp = int(cols[1].text.replace(\"째C\", \"\"))  # Convert to integer\n",
    "    condition = cols[2].text\n",
    "    weather_data.append({\"day\": day, \"temperature\": temp, \"condition\": condition})\n",
    "\n",
    "for entry in weather_data:\n",
    "    print(f\"{entry['day']}: {entry['temperature']}째C, {entry['condition']}\")\n",
    "\n",
    "# Finding the hottest day\n",
    "hottest_day = max(weather_data, key=lambda x: x[\"temperature\"])\n",
    "print(f\"\\nHottest day: {hottest_day['day']} with {hottest_day['temperature']}째C\")\n",
    "\n",
    "# Finding all Sunny days\n",
    "sunny_days = [entry[\"day\"] for entry in weather_data if entry[\"condition\"] == \"Sunny\"]\n",
    "print(f\"Sunny days: {', '.join(sunny_days)}\")\n",
    "\n",
    "# Compute average temperature\n",
    "average_temp = sum(entry[\"temperature\"] for entry in weather_data) / len(weather_data)\n",
    "print(f\"Average temperature for the week: {average_temp:.2f}째C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"jobs.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS jobs (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    title TEXT,\n",
    "    company TEXT,\n",
    "    location TEXT,\n",
    "    description TEXT,\n",
    "    apply_link TEXT,\n",
    "    UNIQUE (title, company, location)\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "url = \"https://realpython.github.io/fake-jobs\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "job_listings = soup.find_all(\"div\", class_=\"card-content\")\n",
    "\n",
    "for job in job_listings:\n",
    "    title = job.find(\"h2\", class_=\"title\").text.strip()\n",
    "    company = job.find(\"h3\", class_=\"company\").text.strip()\n",
    "    location = job.find(\"p\", class_=\"location\").text.strip()\n",
    "    description = job.find(\"div\", class_=\"description\").text.strip()\n",
    "    apply_link = job.find(\"a\")[\"href\"]\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "    INSERT INTO jobs (title, company, location, description, apply_link) \n",
    "    VALUES (?, ?, ?, ?, ?) \n",
    "    ON CONFLICT(title, company, location) \n",
    "    DO UPDATE SET description=excluded.description, apply_link=excluded.apply_link\n",
    "    \"\"\", (title, company, location, description, apply_link))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Job listings scraped and stored successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
